<!doctype html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>HAQAGen | Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization</title>
  <meta name="description" content="Project page for Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization (WACV 2026). HAQAGen: resolution-invariant NIR-to-RGB colorization with histogram-aware supervision, HSV-SPADE priors, and adaptive-resolution inference." />
  <meta property="og:title" content="HAQAGen | Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization" />
  <meta property="og:description" content="Resolution-invariant NIR image colorization with histogram-aware supervision, HSV-SPADE priors, and adaptive-resolution inference." />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="assets/fig2.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link rel="icon" href="assets/favicon.svg" type="image/svg+xml" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Space+Grotesk:wght@500;700&display=swap" rel="stylesheet" />

  <!-- KaTeX for equations -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9GgmY/EbD1Y8b7Z1F" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>

  <link rel="stylesheet" href="assets/styles.css" />
</head>

<body>
  <div class="bg">
    <div class="bg__grid"></div>
    <div class="bg__glow bg__glow--a"></div>
    <div class="bg__glow bg__glow--b"></div>
    <div class="bg__grain"></div>
  </div>

  <header class="nav">
    <div class="container nav__inner">
      <a class="brand" href="#top">
        <span class="brand__mark">HAQAGen</span>
        <span class="brand__sub">NIR → RGB • WACV 2026</span>
      </a>

      <nav class="nav__links" aria-label="Primary">
        <a href="#abstract">Abstract</a>
        <a href="#method">Method</a>
        <a href="#inference">Adaptive inference</a>
        <a href="#results">Results</a>
        <a href="#citation">Citation</a>
      </nav>

      <div class="nav__actions">
        <button class="iconbtn" id="themeToggle" type="button" aria-label="Toggle theme">
          <svg viewBox="0 0 24 24" fill="none" aria-hidden="true">
            <path d="M12 18a6 6 0 1 0 0-12 6 6 0 0 0 0 12Z" stroke="currentColor" stroke-width="1.8"/>
            <path d="M12 2v2M12 20v2M4 12H2M22 12h-2M4.9 4.9l1.4 1.4M17.7 17.7l1.4 1.4M19.1 4.9l-1.4 1.4M6.3 17.7l-1.4 1.4" stroke="currentColor" stroke-width="1.8" stroke-linecap="round"/>
          </svg>
        </button>
        <button class="iconbtn nav__burger" id="burger" type="button" aria-label="Open menu">
          <span></span><span></span><span></span>
        </button>
      </div>
    </div>

    <div class="nav__mobile" id="mobileNav" hidden>
      <a href="#abstract">Abstract</a>
      <a href="#method">Method</a>
      <a href="#inference">Adaptive inference</a>
      <a href="#results">Results</a>
      <a href="#citation">Citation</a>
    </div>
  </header>

  <main id="top">
    <section class="hero container">
      <div class="hero__copy reveal">
        <p class="pill">Accepted to <strong>WACV 2026</strong></p>
        <h1 class="hero__title">Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization</h1>
        <p class="hero__authors">Abhinav Attri*, Rajeev Ranjan Dwivedi*, Samiran Das, Vinod Kumar Kurmi</p>
        <p class="hero__affil">Indian Institute of Science Education and Research Bhopal, India · <span class="mono">{abhinav21, rajeev22, samiran, vinodkk}@iiserb.ac.in</span></p>

        <p class="hero__tagline">
          HAQAGen is a unified generative model for <strong>resolution-invariant</strong> NIR-to-RGB colorization
          that balances <strong>chromatic realism</strong> with <strong>structural fidelity</strong>.
        </p>

        <div class="cta">
          <a class="btn btn--primary" href="assets/paper.pdf" target="_blank" rel="noopener">Paper (PDF)</a>
          <a class="btn" href="https://github.com/<your-org-or-username>/HAQAGen" target="_blank" rel="noopener">Code (edit link)</a>
          <a class="btn" href="https://www.youtube.com/watch?v=<video_id>" target="_blank" rel="noopener">Video (edit link)</a>
        </div>

        <div class="kpis">
          <div class="kpi">
            <div class="kpi__num">24.96</div>
            <div class="kpi__lbl">PSNR on VCIP2020</div>
          </div>
          <div class="kpi">
            <div class="kpi__num">0.18</div>
            <div class="kpi__lbl">LPIPS on VCIP2020</div>
          </div>
          <div class="kpi">
            <div class="kpi__num">Adaptive</div>
            <div class="kpi__lbl">Native-resolution inference</div>
          </div>
        </div>
      </div>

      <div class="hero__media reveal">
        <figure class="figure">
          <img src="assets/fig2.png" alt="Qualitative comparison of NIR input, ground truth, baseline (ColorMamba), and HAQAGen." class="figure__img js-lightbox" />
          <figcaption class="figure__cap">
            Qualitative comparison: HAQAGen preserves texture and produces more natural colors, especially when avoiding global resizing.
          </figcaption>
        </figure>
      </div>
    </section>

    <section id="abstract" class="section container">
      <div class="section__head reveal">
        <h2>Abstract</h2>
        <p class="muted">
          NIR cameras see what visible-light cameras often miss, but the output is not human-friendly.
          HAQAGen tackles the NIR-to-RGB translation problem with global color statistics, local hue priors, and resolution-aware inference.
        </p>
      </div>

      <div class="card reveal">
        <p class="lead">We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature- based similarity to preserve texture information, (ii) lo- cal hue–saturation priors injected via Spatially Adap- tive Denormalization (SPADE) to stabilize chromatic re- construction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consis- tency, while scaling to native resolutions without compro- mising texture fidelity or generalization. Extensive evalu- ations on FANVID, OMSIV, VCIP2020, and RGB2NIR us- ing different evaluation metrics demonstrate consistent im- provements over state-of-the-art baseline methods. HAQA- Gen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective so- lution for NIR-to-RGB translation across diverse imaging scenarios.</p>
      </div>

      <div class="grid2">
        <div class="card reveal">
          <h3>What makes NIR-to-RGB hard?</h3>
          <ul class="bullets">
            <li><strong>Spectral ambiguity:</strong> NIR intensity does not uniquely determine visible color.</li>
            <li><strong>Texture vs color trade-off:</strong> good structure can still come with hue drift or tinting.</li>
            <li><strong>Resolution mismatch:</strong> training at fixed sizes can blur details when deployed on high-res imagery.</li>
          </ul>
        </div>

        <div class="card reveal">
          <h3>What HAQAGen adds</h3>
          <ul class="bullets">
            <li><strong>Histogram-assisted supervision:</strong> differentiable CDF matching aligns global color statistics.</li>
            <li><strong>HSV-SPADE priors:</strong> local hue and saturation guide decoding where NIR alone is ambiguous.</li>
            <li><strong>Texture-aware learning:</strong> feature-space constraints preserve fine details.</li>
            <li><strong>Adaptive-resolution inference:</strong> sliding-window patching avoids resize blur.</li>
          </ul>
        </div>
      </div>
    </section>

    <section id="method" class="section container">
      <div class="section__head reveal">
        <h2>Method</h2>
        <p class="muted">
          A dual-branch generator built on a Mamba-based encoder-decoder (ColorMamba), trained with multi-space adversarial critics
          and a reconstruction loss that simultaneously targets texture, semantics, and global color statistics.
        </p>
      </div>

      <div class="grid2">
        <figure class="figure card reveal">
          <img src="assets/fig1.png" alt="Framework diagram: NIR features feed an HSV predictor and RGB reconstruction network, with SPADE-modulated decoding." class="figure__img js-lightbox" />
          <figcaption class="figure__cap">
            Framework overview. NIR features feed an HSV predictor and an RGB reconstruction branch. HSV priors guide decoding via SPADE.
          </figcaption>
        </figure>

        <div class="card reveal">
          <h3>Core building blocks</h3>

          <div class="tabs" role="tablist" aria-label="Method components">
            <button class="tab is-active" role="tab" aria-selected="true" data-tab="t1">Dual-branch generator</button>
            <button class="tab" role="tab" aria-selected="false" data-tab="t2">HSV-SPADE conditioning</button>
            <button class="tab" role="tab" aria-selected="false" data-tab="t3">Histogram + texture loss</button>
            <button class="tab" role="tab" aria-selected="false" data-tab="t4">Two discriminators</button>
          </div>

          <div class="tabpanes">
            <div class="tabpane is-active" id="t1">
              <p>
                The generator uses a shared encoder and decoder (ColorMamba backbone) with two heads:
                an RGB reconstruction branch \(G_A\) and an HSV-prior branch \(G_B\).
              </p>
              <p class="muted">
                Predicting HSV provides a compact way to represent chromatic intent and to spot hue failures even when luminance looks plausible.
              </p>
            </div>

            <div class="tabpane" id="t2">
              <p>
                The predicted HSV field modulates decoder features through SPADE-style affine transforms:
              </p>
              <div class="math">
                \( \hat{F} = \gamma(\hat{y}_{hsv}) \odot F + \beta(\hat{y}_{hsv}) \)
              </div>
              <p class="muted">
                This injects local hue and saturation priors into the RGB decoding pathway, improving local chromatic consistency.
              </p>
            </div>

            <div class="tabpane" id="t3">
              <p>
                The reconstruction term is feature- and statistics-aware:
              </p>
              <div class="math">
                \( \mathcal{L}_{rec}(\hat{y},y)=
                \alpha\lVert f(\hat{y})-f(y)\rVert_2^2
                +\gamma(1-\cos(f(\hat{y}),f(y)))
                +\beta\lVert CDF(\hat{y})-CDF(y)\rVert_1
                +\delta\lVert g(\hat{y})-g(y)\rVert_2^2 \)
              </div>
              <p class="muted">
                Weights: \((\alpha,\beta,\gamma,\delta)=(1.0,1.5,1.0,0.2)\).
                Here \(f\) is a frozen 4-layer autoencoder (task-specific texture basis) and \(g\) uses VGG-19 relu4_2 features.
                The differentiable CDF term uses a soft histogram with 64 bins and temperature \(\tau=0.02\).
              </p>
            </div>

            <div class="tabpane" id="t4">
              <p>
                Two PatchGAN critics operate in <strong>RGB</strong> and <strong>HSV</strong> spaces.
                This enforces complementary constraints on luminance and chrominance, using a hinge adversarial loss (70×70 receptive fields),
                spectral normalization, and a 1:1 generator-to-discriminator update ratio.
              </p>
              <p class="muted">
                Distinct critics make hue failures more detectable even when structure looks right.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="callout reveal">
        <div class="callout__title">Training and implementation details</div>
        <div class="callout__body">
          Training uses mixed precision (AMP, fp16) with a global batch size of 16 across four RTX 4090 GPUs.
          Loss weights for the composite objective are set to \(\lambda_{MSE}: \lambda_{feat}: \lambda_{adv} = 15:15:1\).
        </div>
      </div>
    </section>

    <section id="inference" class="section container">
      <div class="section__head reveal">
        <h2>Adaptive-resolution inference</h2>
        <p class="muted">
          Instead of resizing everything to a fixed training size (which can blur high-frequency detail), HAQAGen can run at native resolution
          using sliding-window patching with feathered blending.
        </p>
      </div>

      <div class="grid2">
        <figure class="figure card reveal">
          <img src="assets/fig3.png" alt="Adaptive patching diagram showing tiled inference and stitching patches." class="figure__img js-lightbox" />
          <figcaption class="figure__cap">
            Adaptive patching: stride-based tiling, patch-wise colorization, and feathered stitching for seamless RGB output.
          </figcaption>
        </figure>

        <div class="card reveal">
          <h3>How it works</h3>
          <ul class="bullets">
            <li>Patch size \(P=256\).</li>
            <li>Stride \(S \in \{222, 240\}\) giving overlaps of roughly 16 to 34 px.</li>
            <li>Hanning feather masks blend patch borders to avoid seams.</li>
            <li>Reflective padding handles small borders cleanly.</li>
          </ul>

          <p class="muted">
            The goal is simple: keep the model in its comfort zone (a fixed patch size) while preserving details that global resizing would destroy.
          </p>
        </div>
      </div>

      <figure class="figure reveal">
        <img src="assets/fig5.png" alt="OMSIV qualitative gallery comparing resizing versus adaptive inference." class="figure__img js-lightbox" />
        <figcaption class="figure__cap">
          OMSIV examples: sliding-window inference preserves texture and tone continuity at high resolution, outperforming global resizing.
        </figcaption>
      </figure>
    </section>

    <section id="results" class="section container">
      <div class="section__head reveal">
        <h2>Results</h2>
        <p class="muted">
          Evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR show consistent improvements over state-of-the-art baselines,
          with especially strong gains on perceptual quality.
        </p>
      </div>

      <div class="grid2">
        <div class="card reveal">
          <h3>Key quantitative takeaways</h3>
          <p>Our model achieves the best PSNR (24.96 dB) and the lowest LPIPS (0.18), while matching the top SSIM (0.71). Although AE is marginally higher than ColorMamba (2.96 vs 2.81), visual inspection indicates that this trade-off correlates with richer chroma and sharper textures. Across the broader set of baselines, HAQAGen reduces AE by at least 23.3% (vs. SST) and LPIPS by 34.6% (vs. NIR-GNN), indicating strong perceptual fidelity.</p>

          <div class="miniKpis">
            <div class="miniKpi"><span class="miniKpi__k">PSNR</span><span class="miniKpi__v">24.96</span></div>
            <div class="miniKpi"><span class="miniKpi__k">SSIM</span><span class="miniKpi__v">0.71</span></div>
            <div class="miniKpi"><span class="miniKpi__k">AE</span><span class="miniKpi__v">2.96</span></div>
            <div class="miniKpi"><span class="miniKpi__k">LPIPS</span><span class="miniKpi__v">0.18</span></div>
          </div>
        </div>

        <div class="card reveal">
          <h3>What looks better (and why)</h3>
          <ul class="bullets">
            <li>Texture fidelity: fine details like foliage, contours, and fabric are better preserved, with less oversmoothing.</li>
<li>Chromatic realism: the CDF prior curbs tinting and encourages natural tonal distributions across materials.</li>
<li>Edge consistency: boundaries at depth changes stay aligned after colorization, suggesting SPADE-conditioned decoding improves local hue assignment.</li>
          </ul>
        </div>
      </div>

      <figure class="figure reveal">
        <img src="assets/fig4.png" alt="Qualitative comparison on VCIP2020 across multiple methods." class="figure__img js-lightbox" />
        <figcaption class="figure__cap">
          VCIP2020 qualitative comparison: HAQAGen produces sharper textures and more natural chromatic distributions.
        </figcaption>
      </figure>

      <div class="section__subhead reveal">
        <h3>Datasets</h3>
        <p class="muted">Dataset statistics and splits used in the paper.</p>
      </div>

      <div class="tableWrap reveal">
        <table class="table">
          <thead>
            <tr>
              <th>Dataset</th>
              <th>Type</th>
              <th>#Pairs</th>
              <th>Train / Val / Test</th>
              <th>Modal resolution</th>
              <th>Bit depth</th>
              <th>Year</th>
            </tr>
          </thead>
          <tbody>
            <tr><td>VCIP2020</td><td>indoor/outdoor</td><td>400</td><td>320 / 40 / 40</td><td>256 × 256</td><td>8</td><td>2020</td></tr>
<tr><td>FANVID</td><td>faces &amp; urban</td><td>5144</td><td>4100 / 514 / 530</td><td>2048 × 1536</td><td>8</td><td>2024</td></tr>
<tr><td>OMSIV</td><td>outdoor</td><td>532</td><td>426 / 53 / 53</td><td>580 × 320</td><td>8</td><td>2017</td></tr>
<tr><td>RGB2NIR</td><td>mixed scenes</td><td>477</td><td>382 / 48 / 47</td><td>var. (≤ 1024 × 768)</td><td>16</td><td>2011</td></tr>
          </tbody>
        </table>
      </div>

      <div class="section__subhead reveal">
        <h3>Ablations</h3>
        <p class="muted">How different pieces of the training objective and architecture affect performance (VCIP2020).</p>
      </div>

      <div class="grid2">
        <div class="tableWrap reveal">
          <table class="table">
            <thead>
              <tr><th colspan="4">Reconstruction loss variants</th></tr>
              <tr>
                <th>Loss variant</th><th>PSNR ↑</th><th>SSIM ↑</th><th>AE ↓</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>MSE + Cosine (ColorMamba)</td><td>24.56</td><td>0.71</td><td>2.81</td></tr>
<tr><td>+ VGG perceptual</td><td>23.63</td><td>0.70</td><td>4.32</td></tr>
<tr><td>+ Histogram only</td><td>23.81</td><td>0.68</td><td>3.66</td></tr>
<tr><td>+ Texture (f) only</td><td>24.12</td><td>0.69</td><td>3.01</td></tr>
<tr><td>Full L_rec (ours)</td><td>24.96</td><td>0.71</td><td>2.96</td></tr>
            </tbody>
          </table>
        </div>

        <div class="tableWrap reveal">
          <table class="table">
            <thead>
              <tr><th colspan="4">HSV-SPADE branch</th></tr>
              <tr>
                <th>Variant</th><th>PSNR ↑</th><th>SSIM ↑</th><th>AE ↓</th>
              </tr>
            </thead>
            <tbody>
              <tr><td>Without HSV-SPADE branch</td><td>24.21</td><td>0.69</td><td>3.52</td></tr>
<tr><td>With HSV-SPADE (ours)</td><td>24.96</td><td>0.71</td><td>2.96</td></tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>

    <section id="citation" class="section container">
      <div class="section__head reveal">
        <h2>Citation</h2>
        <p class="muted">
          If you use this work, please cite the paper. Update fields like pages once the official proceedings entry is available.
        </p>
      </div>

      <div class="card reveal">
        <div class="row">
          <pre class="code" id="bibtex">@inproceedings{attri2026haqagen,
  title={Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization},
  author={Attri, Abhinav and Dwivedi, Rajeev Ranjan and Das, Samiran and Kurmi, Vinod Kumar},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year={2026}
}</pre>
          <div class="row__actions">
            <button class="btn btn--small" id="copyBib">Copy BibTeX</button>
            <a class="btn btn--small" href="assets/paper.pdf" target="_blank" rel="noopener">Open PDF</a>
          </div>
        </div>
      </div>

      <div class="card reveal">
        <h3>Contact</h3>
        <p class="muted">
          For questions or collaborations, reach out to the authors at <span class="mono">{abhinav21, rajeev22, samiran, vinodkk}@iiserb.ac.in</span>.
        </p>
      </div>
    </section>
  </main>

  <footer class="footer">
    <div class="container footer__inner">
      <div>
        <div class="footer__title">HAQAGen · WACV 2026</div>
        <div class="footer__sub muted">Project page template, built as a single static site for GitHub Pages.</div>
      </div>
      <div class="footer__links">
        <a href="#top">Back to top</a>
        <a href="assets/paper.pdf" target="_blank" rel="noopener">PDF</a>
        <a href="https://github.com/<your-org-or-username>/HAQAGen" target="_blank" rel="noopener">Code</a>
      </div>
    </div>
  </footer>

  <div class="lightbox" id="lightbox" hidden>
    <button class="lightbox__close" id="lightboxClose" aria-label="Close image">×</button>
    <img class="lightbox__img" id="lightboxImg" alt="" />
    <div class="lightbox__cap" id="lightboxCap"></div>
  </div>

  <script src="assets/script.js"></script>
  <script>
    window.addEventListener("DOMContentLoaded", () => {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "\\(", right: "\\)", display: false}
        ]
      });
    });
  </script>
</body>
</html>
